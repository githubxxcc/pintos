+--------------------+
| PROJECT 1: THREADS |
|   DESIGN DOCUMENT  |
+--------------------+

ALARM CLOCK
===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

<thread.h>
struct thread {
+    int64_t wake_up_time;               /* Wake up time from sleep */
+    struct list_elem sleep_elem;        /* Sleep List element */
}

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

It will put the current thread to the wait_list;
then blocked the current thread by calling thread_block();

thread_sleep():
- get current thread
- disable interrupt 
- calculate wake up ticks 
- push the thread to the sleep_list
- thread_block()
- [after returned]
- enable interrupt



>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

Nothing extra is added in the timer interrupt handler. 

In schedule(), The sleep_list is sorted, with the earliest ot be wake up at 
the begining. So at each timer interrupt, it is not needed to iterate through 
the whole list. 


---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
Interrupt is disabled before any critical instructions. So only one thread is 
allowed to access the wait_thread and modify it at one time.

[Question]
However, in a SMP multithreaded machine, synchronization on the sleep_list is
needed if it is a global object? 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupt will be disabled in critical steps. 


---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Alternative 1:
Leave the thread in the ready_list, and check each of them when considering 
which one to run by the scheduler. 
--
Current implementation is better in this design since it does not mess up with
the ready_list. 

Alternative 2: 
Use an unsorted sleep_list. This will result O(n) in every single invocation 
of the timer interrupt. 
--
Current implementation has greater cost on inserting element into the list. 
But allows O(1) for removal and avoids unnecessary iterations during timer 
interrupt. So it will be better with more frequent timer interrupts and less 
invocations of timer_sleep()

Alternative 3: 
Waking up threads in every timer interrupt. 
--
This will incurs greater cost from checking the sleep_list more frequently. 


PRIORITY SCHEDULING
===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

<thread.h>
struct thread {
+    struct lock *waiting_lock;          /* Lock which the thread is waiting for */
+    struct list donors;                 /* doners list wiht priority higher than t. */
+    struct list_elem donor_elem;        /* List element for donors list, one list only*/
+    struct list_elem waiter_elem;       /* List element for waiter list,one list only*/ 
+    int static_priority;                /* Priority not affected by donation */
}
-----
/* Thread list types */
+enum list_type
+{
+    ELEM,               /* For ready list scheduling in struct thread */
+    DONOR,              /* For donor list in struct thread*/
+    WAITER              /* For waiter list in struct semaphore */
+};

<synch.h>
/* One semaphore in a list. */
+struct semaphore_elem 
+{
+    struct list_elem elem;              /* List element. */
+    struct semaphore semaphore;         /* This semaphore. */
+    struct thread * t;                  /* Thread waiting on this semaphore */
+};


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
        >> .png file.)
1. Thread TL running, acquiring Lock L1


     ╔══════════════════╗
     ║                  ║
     ║ TL               ║
     ║ priority = 30    ║
     ║ static_pri = 30  ║        ╔══════════╗
     ║                  ║        ║L1        ║
     ║ waiting_lock=null║        ║waiters=[]║
     ║ donors = []      ║        ║holder= TL║
     ╚══════════════════╝        ╚════|═════╝
             ^------------------------+
                  acquired by

 
2. Thread TM running, acquiring Lock L2, blocked on acquiring Lock L1.
   Thread TM donates to TL.

     ╔═══════════════╗          ╔═════════════════╗
     ║TM             ║          ║TL               ║
     ║               ║          ║                 ║
     ║pri = 31       ║          ║pri=31           ║
     ║static_pri = 31║ donates  ║static_pri = 30  ║
     ║               ║--------> ║                 ║
     ║waiting_lock=L1║          ║waiting_lock=null║
     ║donors=[]      ║-------+  ║donors=[TM]      ║
     ║               ║       |  ║                 ║
     ╚═══════════════╝       |  ╚═════════════════╝
              ^              |        ^
              | acquired by  |        |acquired by
         ╔══════════╗        |   ╔══════════╗    
         ║L2        ║   block|   ║L1        ║    
         ║waiters   ║   on   |   ║waiters   ║    
         ║ = []     ║        +-> ║ = [TM]   ║    
         ║holder= TM║            ║holder= TL║    
         ╚══════════╝            ╚══════════╝    




3. Thread TH running, blocked on acquiring lock L2.
   Thread TH donates to TM;
   TM blocks -> TH's priority propogated to TL -> TL resets priority
 ╔═══════════════╗      ╔═══════════════╗      ╔═══════════════╗
 ║TH             ║      ║TM             ║      ║TL             ║
 ║               ║      ║               ║      ║               ║
 ║pri = 32       ║donate║pri = 32       ║donate║pri = 32       ║
 ║static_pri = 32║----> ║static_pri = 31║----->║static_pri = 30║
 ║               ║      ║               ║      ║               ║
 ║waiting_lock=L2║      ║waiting_lock=L1║      ║waiting_lock=L1║
 ║donors=[]      ║      ║donors=[TH]    ║      ║donors=[TM]    ║
 ║               ║      ║               ║      ║               ║
 ╚═══════════════╝      ╚═══════════════╝      ╚═══════════════╝
           |         acquired^    |blocks             ^ acquired by
    blocks |         by      |    |on                 |
    on     +--+          +---+    +----+              |
              v          |             v              |
          ╔══════════╗   |       ╔══════════╗         |
          ║L2        ║   |       ║L1        ║         |
          ║waiters   ║---+       ║waiters   ║ --------+
          ║ = [TH]   ║           ║ = [TM]   ║
          ║holder= TM║           ║holder= TL║
          ╚══════════╝           ╚══════════╝




---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

The waiters list of the semaphore will be sorted with priority in descending order. 
So the removal from the front will always be the one with the highest priority. 


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

1. IF NOT a free lock:
2.   recursively donate priority to holder, and holder's donees as well.
     (however, donor-donee relationship is not propogated recursively)
3. ELSE:
4.    do nothing 
5.Sema_down on lock's semaphore
(Got the lock)
6. Set the lock's holder

** Handling nested donation 
priority donation will be propogated through the holder's chain. The current
lock holder will have its priority changed.   
If the lock holder is locked by another lock LK, LK's current holder will 
recursively change its priority as well. 
If the lock holder is currently in the ready list, it needs to be re-queued 
to reflect change of the priority.


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

a. When higher-priority thread is waiting for the lock
1. Remove all those threads waiting for the same lock(aka, threads in the waiter
   list of the semaphore) from the thread's donors
2. Wake up a thread 
(It might get yielded before returning from sema_up) 

b. When lower-priority thread is waiting for it. 
Same thing happens.

c. When no thread waiting for it. 
Then no thread will be deleted from its donors list


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

thread_set_priority() will cause a re-ordering of the ready_list. 
A lock on the read_list could be used to synchronize the modifications. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The tracking of ready threads in the run queue could be further improved
with a bit vector that tracks the emptiness of the queue. Also the number of
ready threads could be tracked by a count, which needs to be syncronized if 
there are multiple threads running at the same time. (eg, multi-core processors)

ADVANCED SCHEDULER
==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.



-----
/* For fix point calculation */
<fpoint.h>
+#ifndef __LIB_FPOINT_H
+#define __LIB_FPOINT_H
+
+#include <stdint.h>
+
+/* F_PAD as offset for conversion between integer and fix point number */
+#define F_PAD (1<<14)
+
+#define F_TOFPOINT(x) ((x)*F_PAD)
+#define F_TOINT_ZERO(x) ((x) / F_PAD)
+#define F_TOINT_NEAR(x) ((x) >= 0 ? ((x)+F_PAD/2)/F_PAD : ((x)-F_PAD/2)/F_PAD)
+#define F_ADD(x,y) ((x)+(y))
+#define F_SUBTRACT(x,y) ((x)-(y))
+#define F_ADD_INT(x,n) ((x)+(n)*F_PAD)
+#define F_SUBTRACT_N(x,n) ((x)-(n)*F_PAD)
+#define F_MULTIPLE(x,y) ((int64_t) (x) * (y) / F_PAD)
+#define F_MULTIPLE_INT(x,n) ((x) * (n))
+#define F_DIVIDE(x, y) ((int64_t) (x) * F_PAD / (y))
+#define F_DIVIDE_INT(x, n) ((x) / (n))
+
+#endif

----
<thread.h>
+/* Thread niceness. */
+#define NICE_DEFAULT 0                  /* Default nice. */
+#define NICE_MIN -20                    /* Lowest nice. */
+#define NICE_MAX 20                     /* Highest nice. */

struct thread{
+    int32_t recent_cpu;                 /* Per-thread recent_cpu data */
+    int nice;                           /* What a good guy */
+    bool recent_cpu_dirty;              /* Whether recent_cpu has changed */
}


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes. For example, the sequence of each update at each time tick. 
Is load_avg updated before recent_cpu? 

My rule follows as such:
laod_avg -> all recent_cpu -> all priority -> current recent_cpu


>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
Following the original design. Calculation happens at each timer tick. 
However, the ready threads number could be updated outside of interrupt
context, which is an optimization can be made to shorten the time spent
in the context.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

- Improving on the data strucutre used to track the run queue is 
  is definitely one optimization can be made. With a 64 bit int
  tracking the emptiness of each run queue should improve time spent on
  finding out the maximal priority

- Improving on the tracking of ready counts will be another. 
  It was not implemented due ot extra complixity in initiation the thread
  system. 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

- Macros are used to perform the fixed-point math in fpoint.h. Since 
  each operation is relatively simple. Macros are faster than functions
  since they do not invoke function calling routines.

